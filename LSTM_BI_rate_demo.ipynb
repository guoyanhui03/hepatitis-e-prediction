{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "set params:",
   "id": "d5791e2403d28938"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T15:24:16.735281Z",
     "start_time": "2024-09-08T15:24:15.025975Z"
    }
   },
   "source": [
    "import torch\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataFile = './dataset/发病率原始数据.xlsx'\n",
    "        self.gpu = 0  # gpu 卡号\n",
    "        self.epochs = 200  # 训练轮数\n",
    "        self.num_layers = 1  # LSTM层数\n",
    "        self.input_size = 2  # 输入特征的维度\n",
    "        self.hidden_size = 50  # LSTM节点个数\n",
    "        self.output_size = 1 # LSTM输出维度\n",
    "        self.lr = 0.001 #learning rate 学习率\n",
    "        self.sequence_length = 4  # sequence的长度，默认是用前四个月预测下一个月\n",
    "        self.batch_size = 8 \n",
    "        self.useGPU = True  # 默认值\n",
    "        self.dropout = 0.15\n",
    "        self.save_file = 'model/LSTM.pth'  # 默认值\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "args = Args()  # 创建参数对象\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "create dataset:\n",
   "id": "fe7ae08f340b0151"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:24:22.750245Z",
     "start_time": "2024-09-08T15:24:22.741248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, xx, yy, transform=None):\n",
    "        self.x = torch.tensor(xx, dtype=torch.float32)  # 特征数据\n",
    "        self.y = torch.tensor(yy, dtype=torch.float32).unsqueeze(1)  # 标签数据\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.x[index]  # 获取指定索引的特征数据\n",
    "        y1 = self.y[index]  # 获取指定索引的标签数据\n",
    "        return x1, y1  # 否则，直接返回数据\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)  # 返回数据集的长度"
   ],
   "id": "bf01a941756ab1da",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:24:24.881437Z",
     "start_time": "2024-09-08T15:24:24.427337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pandas import read_excel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def getFabingShuData(dataFile, sequence_length, batchSize):   # 单因素预测数据集制作，发病数和发病率都用一个函数\n",
    "    #fabingshu_data = read_excel('./dataset/发病率原始数据.xlsx')\n",
    "    fabingshu_data = read_excel(dataFile)\n",
    "    #fabingshu_data = fabingshu_data['发病率'] # 发病率或是发病数\n",
    "    fabingshu_data = fabingshu_data.iloc[:,1]\n",
    "    fabingshu_max = fabingshu_data.max()  # 收盘价的最大值\n",
    "    fabingshu_min = fabingshu_data.min()  # 收盘价的最小值\n",
    "    fabingshu_data = (fabingshu_data - min(fabingshu_data)) / (max(fabingshu_data) - min(fabingshu_data))\n",
    "\n",
    "    #sequence = 4  # 天数\n",
    "    sequence = sequence_length\n",
    "    X = []  # 特征\n",
    "    Y = []  # 标签\n",
    "\n",
    "    # 遍历数据，创建基于前四个月的数据集\n",
    "    for i in range(sequence, len(fabingshu_data)):\n",
    "        temp_x = fabingshu_data[i - sequence:i]\n",
    "        temp_y = fabingshu_data[i]\n",
    "        #print(temp_x)\n",
    "        #print(temp_y)\n",
    "        X.append(temp_x)\n",
    "        Y.append(temp_y)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "    # 发病率的时候直接使用原始值，发病数的时候，使用标准化的值\n",
    "    Y = Y*(fabingshu_max - fabingshu_min)+fabingshu_min   # 原始值\n",
    "\n",
    "    # 构建训练集和测试集\n",
    "    total_len = X.shape[0]  # 总数据量\n",
    "\n",
    "    # print(total_len)\n",
    "    trainx, trainy = X[:total_len - 30], Y[:total_len - 30]  # 训练集\n",
    "    index = list(range(len(trainx)))\n",
    "    random.shuffle(index)\n",
    "\n",
    "    trainx, trainy = trainx[index[:int(len(index) * 0.90)]], trainy[index[:int(len(index) * 0.90)]]\n",
    "\n",
    "    testx, testy = X[-30:], Y[-30:]  # 测试集\n",
    "    #print(trainx.shape, trainy.shape, testx.shape, testy.shape)\n",
    "\n",
    "    train_loader = DataLoader(dataset=Mydataset(trainx, trainy), batch_size=batchSize,\n",
    "                              shuffle=True)\n",
    "    test_loader = DataLoader(dataset=Mydataset(testx, testy), batch_size=batchSize, shuffle=False)  # 测试集数据加载器\n",
    "    return fabingshu_max, fabingshu_min, train_loader, test_loader  # 返回最大值、最小值、训练集和测试集数据加载器\n",
    "\n",
    "def getFabingBaiduData(dataFile, sequence_length, batchSize):\n",
    "    # fabingshu_data = read_excel('./dataset/发病率原始数据.xlsx')\n",
    "    fabingshu_data = read_excel(dataFile)\n",
    "    #fabingshu_data = fabingshu_data[['发病率','处理后百度指数']]\n",
    "    fabingshu_data = fabingshu_data.iloc[:,1:3]\n",
    "\n",
    "    fabingshu_max = fabingshu_data.iloc[:,0].max()  # 发病最大值\n",
    "    fabingshu_min = fabingshu_data.iloc[:,0].min()  # 发病最小值\n",
    "    fabingshu_data = fabingshu_data.apply(lambda x: (x - min(x)) / (max(x) - min(x)))\n",
    "\n",
    "    #sequence = 4  # 天数\n",
    "    sequence = sequence_length\n",
    "    X = []  # 特征\n",
    "    Y = []  # 标签\n",
    "\n",
    "    # 遍历数据，创建基于前四个月的数据集\n",
    "    for i in range(sequence, len(fabingshu_data)):\n",
    "        temp_x = fabingshu_data.iloc[i - sequence:i, ]\n",
    "        temp_y = fabingshu_data.iloc[i,0]\n",
    "        #print(temp_x)\n",
    "        #print(temp_y)\n",
    "        X.append(temp_x)\n",
    "        Y.append(temp_y)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "\n",
    "    # 发病率的时候使用该语句，发病数的时候不适用\n",
    "    Y = Y*(fabingshu_max - fabingshu_min)+fabingshu_min\n",
    "\n",
    "    # 构建训练集和测试集\n",
    "    total_len = X.shape[0]  # 总数据量\n",
    "\n",
    "    # print(total_len)\n",
    "    trainx, trainy = X[:total_len - 30], Y[:total_len - 30]  # 训练集\n",
    "    index = list(range(len(trainx)))\n",
    "    random.shuffle(index)\n",
    "\n",
    "    trainx, trainy = trainx[index[:int(len(index) * 0.90)]], trainy[index[:int(len(index) * 0.90)]]\n",
    "\n",
    "    testx, testy = X[-30:], Y[-30:]  # 测试集\n",
    "    #print(trainx.shape, trainy.shape, testx.shape, testy.shape)\n",
    "\n",
    "    train_loader = DataLoader(dataset=Mydataset(trainx, trainy), batch_size=batchSize,\n",
    "                              shuffle=True)\n",
    "    test_loader = DataLoader(dataset=Mydataset(testx, testy), batch_size=batchSize, shuffle=False)  # 测试集数据加载器\n",
    "    return fabingshu_max, fabingshu_min, train_loader, test_loader  # 返回最大值、最小值、训练集和测试集数据加载器"
   ],
   "id": "533741d5e41f7381",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:24:27.149170Z",
     "start_time": "2024-09-08T15:24:27.129166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ],
   "id": "fd7a21240643ebf3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:25:15.882138Z",
     "start_time": "2024-09-08T15:25:15.862143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd  import Variable\n",
    "from LSTMModel import LSTMModel\n",
    "#from parser_lstm import args\n",
    "\n",
    "def model_eval(model,test_loader,single_or_daidu,max_num, min_num):\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds = []  # 初始化预测值列表\n",
    "    labels = []  # 初始化真实标签值列表\n",
    "    # Evaluate data for one epoch\n",
    "    for idx, (data, label) in enumerate(test_loader):\n",
    "        if args.useGPU:\n",
    "            # 单因素的数据需要升维\n",
    "            if single_or_daidu == 1:\n",
    "                data1 = data.unsqueeze(2)  # 删除data张量中的第一个维度，并将其移动到GPU\n",
    "                pred = model(Variable(data1).cuda())  # 将data1封装成Variable并传入模型进行前向传播，得到预测值\n",
    "            else:\n",
    "                pred = model(Variable(data).cuda())  # 将data1封装成Variable并传入模型进行前向传播，得到预测值\n",
    "            label = label.cuda()  # 将标签数据添加一个维度并移动到GPU\n",
    "            # print(label.shape)\n",
    "        else:\n",
    "            pass\n",
    "        criterion =nn.MSELoss(reduction='sum')\n",
    "        loss = criterion(pred, label)\n",
    "        total_loss += loss.item()\n",
    "        preds.extend(pred.cpu().detach().numpy().tolist())\n",
    "        labels.extend(label.cpu().detach().numpy().tolist())\n",
    "\n",
    "    # # 计算评价指标\n",
    "    preds = np.array(preds)\n",
    "    preds = np.squeeze(preds)\n",
    "    labels = np.array(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    # 如果是发病率，不需要下面两行代码\n",
    "    #preds = preds * (max_num - min_num) + min_num\n",
    "    #labels = labels * (max_num - min_num) + min_num\n",
    "\n",
    "    rmse = np.sqrt(np.mean(np.square(preds - labels)))\n",
    "    mae = np.mean(np.abs(preds - labels))\n",
    "    mape = np.mean(np.abs(preds - labels) / labels) * 100\n",
    "    \n",
    "    return rmse, mae, mape, preds"
   ],
   "id": "38102d9474467768",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:26:26.064382Z",
     "start_time": "2024-09-08T15:26:24.118345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "#from dataset import getFabingShuData,getFabingBaiduData\n",
    "# 设置参数\n",
    "#args.datafile = r'./dataset/发病率原始数据.xlsx' # 发病率或是发病数的文件地址\n",
    "file_path = \"model/CV_LSTM_BD_rate/\"\n",
    "\n",
    "single_or_daidu = 2   # 1 表示单因素，2表示双因素，即加百度指数\n",
    "\n",
    "# 获取测试集的数据\n",
    "if single_or_daidu==1:\n",
    "    max_num,min_num,_, test_loader = getFabingShuData(args.dataFile, args.sequence_length, args.batch_size)\n",
    "elif single_or_daidu==2:\n",
    "    max_num,min_num, _, test_loader = getFabingBaiduData(args.dataFile, args.sequence_length, args.batch_size)\n",
    "results = []\n",
    "evaluations = []\n",
    "for i in range(10):\n",
    "    model = torch.load(file_path+\"LSTM\"+str(i)+\".pth\")\n",
    "    rmse, mae, mape, preds = model_eval(model,test_loader,single_or_daidu,max_num, min_num)\n",
    "\n",
    "    results.append(preds)\n",
    "    evaluations.append([rmse, mae, mape])\n",
    "# print results\n",
    "print('The evaluation results are:')\n",
    "print(evaluations)\n",
    "# 求平均值\n",
    "avgs = np.mean(np.array(evaluations), axis=0)\n",
    "print('The mean of [rmse, mae, mape] is:')\n",
    "print(avgs)\n",
    "# 求方差\n",
    "vars = np.var(np.array(evaluations), axis=0)\n",
    "print('The var of [rmse, mae, mape] is:')\n",
    "print(vars)\n",
    "\n"
   ],
   "id": "e1df0fb9942f5691",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation results are:\n",
      "[[0.017921606988155963, 0.013881135731935501, 15.953784429799287], [0.017411773184841734, 0.013749969750642776, 15.876623509352358], [0.0185042698904796, 0.014141014218330384, 15.489384058951892], [0.017337382805068267, 0.013625359038511912, 15.862123842568703], [0.017962000550797073, 0.013517496113975843, 14.630281865223797], [0.016721162078034986, 0.013345236952106158, 15.143232015860406], [0.016349587438706705, 0.013356693089008331, 15.404897437813975], [0.018820251854090492, 0.014047502229611078, 15.07445144630957], [0.017890118168738808, 0.013809079676866532, 15.237358352065778], [0.017371824390068947, 0.013660895451903344, 14.921989819764244]]\n",
      "The mean of [rmse, mae, mape] is:\n",
      "[1.76289977e-02 1.37134382e-02 1.53594127e+01]\n",
      "The var of [rmse, mae, mape] is:\n",
      "[5.10930284e-07 6.44775549e-08 1.75826950e-01]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ce1a683e9afc25c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
